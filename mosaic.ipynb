{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":9459147,"sourceType":"datasetVersion","datasetId":5750489},{"sourceId":9938099,"sourceType":"datasetVersion","datasetId":6109806},{"sourceId":11361430,"sourceType":"datasetVersion","datasetId":7110930},{"sourceId":13132104,"sourceType":"datasetVersion","datasetId":8319224},{"sourceId":13267324,"sourceType":"datasetVersion","datasetId":8407538},{"sourceId":13274961,"sourceType":"datasetVersion","datasetId":8412607},{"sourceId":555978,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":423031,"modelId":440575},{"sourceId":575765,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":430695,"modelId":447637},{"sourceId":600191,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":449667,"modelId":466045},{"sourceId":612065,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":459778,"modelId":475626},{"sourceId":659925,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":499082,"modelId":514334}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow-addons\n!pip install tfa-nightly\n!pip install scikit-image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport cv2\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils import normalize\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Conv2DTranspose, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Dropout, BatchNormalization, Activation, MaxPool2D, Multiply, GlobalAveragePooling2D, Reshape, Dense, Lambda\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nfrom keras.losses import binary_crossentropy\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve\nimport seaborn as sns\nfrom skimage.restoration import denoise_nl_means, estimate_sigma\nfrom skimage import img_as_ubyte, img_as_float, io\nfrom scipy.ndimage import binary_opening\n\ndef load_and_preprocess_data(folder_path, imagesf, maskf):\n    # Load images\n    image_names = glob.glob(os.path.join(folder_path, imagesf, \"*.png\"))\n    image_names.sort()\n    images = []\n    for image_path in image_names:\n        image = cv2.imread(image_path, 1)\n        if image is not None:\n            images.append(image)\n\n    # Load masks (as color images)\n    mask_names = glob.glob(os.path.join(folder_path, maskf, \"*.png\"))\n    mask_names.sort()\n    masks = []\n    for mask_path in mask_names:\n        mask = cv2.imread(mask_path, cv2.IMREAD_COLOR)  # Load in BGR format\n        if mask is not None:\n            masks.append(mask)\n\n    return images, masks  # No heatmaps loaded here\n\n\ndef create_multiscale_patches(image, large_patch_size, medium_patch_size, small_patch_size, stride):\n    \"\"\"\n    Splits an image into multi-scale patches of given sizes with specified stride.\n\n    :param image: Input image to split into patches.\n    :param large_patch_size: Size of each large patch.\n    :param medium_patch_size: Size of each medium patch.\n    :param small_patch_size: Size of each small patch.\n    :param stride: Number of pixels to move in both horizontal and vertical directions for the next patch.\n    :return: Lists of large, medium, and small patches.\n    \"\"\"\n    large_patches = []\n    medium_patches = []\n    small_patches = []\n\n    h, w = image.shape[:2]\n\n    for y in range(0, h - large_patch_size + 1, stride):\n        for x in range(0, w - large_patch_size + 1, stride):\n            large_patch = image[y:y + large_patch_size, x:x + large_patch_size]\n            large_patches.append(large_patch)\n\n            # Calculate center for medium patch within the large patch\n            m_x_center = x + (large_patch_size - medium_patch_size) // 2\n            m_y_center = y + (large_patch_size - medium_patch_size) // 2\n\n            if m_x_center + medium_patch_size <= w and m_y_center + medium_patch_size <= h:\n                medium_patch = image[m_y_center:m_y_center + medium_patch_size, m_x_center:m_x_center + medium_patch_size]\n                medium_patches.append(medium_patch)\n\n                # Calculate center for small patch within the medium patch\n                l_x_center = m_x_center + (medium_patch_size - small_patch_size) // 2\n                l_y_center = m_y_center + (medium_patch_size - small_patch_size) // 2\n\n                if l_x_center + small_patch_size <= w and l_y_center + small_patch_size <= h:\n                    small_patch = image[l_y_center:l_y_center + small_patch_size, l_x_center:l_x_center + small_patch_size]\n                    small_patches.append(small_patch)\n\n    return large_patches, medium_patches, small_patches\n\n\ndef resize_patches(patches, target_size, is_mask=False):\n    \"\"\"\n    Resize patches to the target size. Uses nearest-neighbor interpolation for masks.\n\n    :param patches: List of patches to resize.\n    :param target_size: Target size for resizing (tuple of width, height).\n    :param is_mask: Boolean indicating if the patches are masks. Defaults to False.\n    :return: Resized patches as a numpy array.\n    \"\"\"\n    interpolation_method = cv2.INTER_NEAREST if is_mask else cv2.INTER_LINEAR\n    resized_patches = [cv2.resize(patch, (target_size, target_size), interpolation=interpolation_method) for patch in patches]\n    return np.array(resized_patches)\n\ndef encode_mask(mask_patches):\n    labelencoder = LabelEncoder()\n    n, h, w = mask_patches.shape  \n    mask_dataset_reshaped = mask_patches.reshape(-1, 1)\n    mask_dataset_reshaped_encoded = labelencoder.fit_transform(mask_dataset_reshaped.ravel())\n    mask_dataset_encoded = mask_dataset_reshaped_encoded.reshape(n, h, w)\n    mask_dataset_encoded = np.expand_dims(mask_dataset_encoded, axis=3)\n    return mask_dataset_encoded, labelencoder  # Return both encoded masks and encoder\n\n\n\ndef categorize_and_reshape_masks(y, n_classes):\n    # Convert masks to one-hot encoded format\n    y_masks_cat = to_categorical(y, num_classes=n_classes)\n    # Reshape the one-hot encoded masks to the desired shape\n    y_cat = y_masks_cat.reshape((y.shape[0], y.shape[1], y.shape[2], n_classes))\n\n    return y_cat","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load and preprocess training data\ntrain_folder = \"/kaggle/input/er-20x-corrected-balanced-centroids/20XCorrectedBalancedER-IHC\"\ntrain_images, train_masks = load_and_preprocess_data(train_folder, \"images\",\"masks\")\n\n\n# Define patch sizes and stride\nlarge_patch_size = 512\nmedium_patch_size = 256\nsmall_patch_size = 192\nstride = 512\n\n# Create multi-scale patches for training data\ntrain_image_large_patches, train_image_medium_patches, train_image_small_patches = [], [], []\ntrain_mask_large_patches, train_mask_medium_patches, train_mask_small_patches = [], [], []\n\nfor image, mask in zip(train_images, train_masks):\n    large_patches, medium_patches, small_patches = create_multiscale_patches(image, large_patch_size, medium_patch_size, small_patch_size, stride)\n    train_image_large_patches.extend(large_patches)\n    train_image_medium_patches.extend(medium_patches)\n    train_image_small_patches.extend(small_patches)\n\n    large_patches, medium_patches, small_patches = create_multiscale_patches(mask, large_patch_size, medium_patch_size, small_patch_size, stride)\n    train_mask_large_patches.extend(large_patches)\n    train_mask_medium_patches.extend(medium_patches)\n    train_mask_small_patches.extend(small_patches)\n\n\n# Convert lists to numpy arrays\ntrain_image_large_patches = np.array(train_image_large_patches)\ntrain_image_medium_patches = np.array(train_image_medium_patches)\ntrain_image_small_patches = np.array(train_image_small_patches)\ntrain_mask_large_patches = np.array(train_mask_large_patches)\ntrain_mask_medium_patches = np.array(train_mask_medium_patches)\ntrain_mask_small_patches = np.array(train_mask_small_patches)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Resize patches\ntrain_image_large_patches_resized = train_image_large_patches  # No need to resize\ntrain_image_medium_patches_resized = resize_patches(train_image_medium_patches, large_patch_size)\ntrain_image_small_patches_resized = resize_patches(train_image_small_patches, large_patch_size)\ntrain_mask_large_patches_resized = train_mask_large_patches  # No need to resize\ntrain_mask_medium_patches_resized = resize_patches(train_mask_medium_patches, large_patch_size, True)  \ntrain_mask_small_patches_resized = resize_patches(train_mask_small_patches, large_patch_size, True)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"large patch shape:\", train_image_large_patches_resized.shape)  \nprint(\"medium shape:\", train_image_medium_patches_resized.shape)   \nprint(\"small shape:\", train_image_small_patches_resized.shape)    \nnum_samples = train_image_large_patches_resized.shape[0]\nprint(\"Number of samples:\", num_samples)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After resizing the patches, concatenate them to form unified representations\nunified_images = np.concatenate([\n    train_image_large_patches_resized,\n    train_image_medium_patches_resized,\n    train_image_small_patches_resized\n], axis=0)\n\nunified_masks_color = np.concatenate([\n    train_mask_large_patches_resized,\n    train_mask_medium_patches_resized,\n    train_mask_small_patches_resized\n], axis=0)\n\n# Verify the shapes\nprint(\"Unified Images shape:\", unified_images.shape) \nprint(\"Unified Masks shape:\", unified_masks_color.shape)   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport cv2\nfrom scipy import ndimage as ndi\n\ndef centroids_dot_mask_from_rgb_mask(mask_rgb, dot_size=3, min_area=5):\n    \"\"\"\n    One 3x3 white dot per nucleus (connected component), regardless of shape/class.\n    0 = background, 1 = centroid.\n\n    Args:\n        mask_rgb: HxWx3 uint8 color mask (0,0,0 background; anything else = nucleus)\n        dot_size: odd integer (3 -> 3x3)\n        min_area: ignore tiny specks smaller than this many pixels\n    \"\"\"\n    H, W, _ = mask_rgb.shape\n\n    # 1) Binary foreground (any non-black pixel is nucleus)\n    fg = (mask_rgb[..., 0] | mask_rgb[..., 1] | mask_rgb[..., 2]) > 0\n    fg_u8 = fg.astype(np.uint8)\n\n    if fg_u8.max() == 0:\n        return np.zeros((H, W), dtype=np.uint8)\n\n    # (Optional) clean up tiny specks that can create fake peaks\n    if min_area > 1:\n        labeled_tmp, n_tmp = ndi.label(fg_u8)\n        sizes = np.bincount(labeled_tmp.ravel())\n        keep = np.ones_like(sizes, dtype=bool)\n        keep[0] = False\n        keep[np.where(sizes < min_area)] = False\n        fg_u8 = keep[labeled_tmp].astype(np.uint8)\n\n    # 2) Distance transform inside foreground\n    dt = cv2.distanceTransform(fg_u8, distanceType=cv2.DIST_L2, maskSize=3)\n\n    # 3) Label connected components (each nucleus)\n    labeled, n = ndi.label(fg_u8)\n\n    # 4) For each component, take the pixel with the maximum distance value\n    dots = np.zeros((H, W), dtype=np.uint8)\n    if n > 0:\n        for lab in range(1, n + 1):\n            mask_lab = (labeled == lab)\n            if not mask_lab.any():\n                continue\n            # argmax within the component\n            flat_idx = np.argmax(dt[mask_lab])\n            r_comp, c_comp = np.where(mask_lab)\n            r = r_comp[flat_idx]\n            c = c_comp[flat_idx]\n            dots[r, c] = 1\n\n    # 5) Expand to a dot_size × dot_size square (clip at borders)\n    k = dot_size // 2\n    if dot_size > 1:\n        kernel = np.ones((dot_size, dot_size), np.uint8)\n        dots = cv2.dilate(dots, kernel, iterations=1)\n\n    return dots.astype(np.uint8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Example usage on three patch scales ===\nunified_heatmaps = []\n\nprint(\"Generating Centroids For Large Patches\")\nfor idx, mask_patch in enumerate(train_mask_large_patches_resized, 1):\n    dots = centroids_dot_mask_from_rgb_mask(mask_patch, 3, 5)\n    unified_heatmaps.append(dots)\n    print(f\"\\rProcessed {idx}/{len(train_mask_large_patches_resized)}\", end='', flush=True)\nprint()\n\nprint(\"Generating Centroids For Medium Patches\")\nfor idx, mask_patch in enumerate(train_mask_medium_patches_resized, 1):\n    dots = centroids_dot_mask_from_rgb_mask(mask_patch, 3, 5)\n    unified_heatmaps.append(dots)\n    print(f\"\\rProcessed {idx}/{len(train_mask_medium_patches_resized)}\", end='', flush=True)\nprint()\n\nprint(\"Generating Centroids For Small Patches\")\nfor idx, mask_patch in enumerate(train_mask_small_patches_resized, 1):\n    dots = centroids_dot_mask_from_rgb_mask(mask_patch, 3, 5)\n    unified_heatmaps.append(dots)\n    print(f\"\\rProcessed {idx}/{len(train_mask_small_patches_resized)}\", end='', flush=True)\nprint()\n\nunified_heatmaps = np.array(unified_heatmaps, dtype=np.uint8)\nprint(\"Done\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unified_masks = np.array([cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY) for mask in unified_masks_color])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Verify the shapes\nprint(\"Unified Images shape:\", unified_images.shape) \nprint(\"Unified Masks shape:\", unified_masks.shape)   \nprint(\"Unified heatmat shape:\", unified_heatmaps.shape) \nprint(\"____________________________________________________\") \nprint(\"Max pixel value in image is: \", unified_images.max())\nprint(\"Max pixel value in heatmap is: \", unified_heatmaps.max())\nprint(\"Min pixel value in heatmap is: \", unified_heatmaps.min())\nprint(\"____________________________________________________\")\nunique_labels = np.unique(unified_masks)\nprint(\"Labels in the mask are : \", unique_labels)\nnum_classes = len(unique_labels)\nprint(\"Total Classes in the mask are : \", num_classes)\nprint(\"____________________________________________________\")\nunique_labels = np.unique(unified_heatmaps)\nprint(\"Labels in the heatmap are : \", unique_labels)\nnum_classesheatmap = len(unique_labels)\nprint(\"Total Classes in the heatmap are : \", num_classesheatmap)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"small_sample = num_samples+num_samples\ndef display_multiscale_pairs(images, masks, heatmaps, base_index):\n    \"\"\"Display large, medium, and small patches for a given base index.\"\"\"\n    fig, axes = plt.subplots(3, 3, figsize=(15, 8))\n    \n    # Large patches (index)\n    axes[0, 0].imshow(images[base_index])\n    axes[0, 0].set_title(f\"Large Image {base_index}\")\n    axes[0, 0].axis('off')\n    \n    axes[1, 0].imshow(masks[base_index], cmap='gray')\n    axes[1, 0].set_title(f\"Large Mask {base_index}\")\n    axes[1, 0].axis('off')\n\n    axes[2, 0].imshow(heatmaps[base_index], cmap='gray')\n    axes[2, 0].set_title(f\"Large heatmaps {base_index}\")\n    axes[2, 0].axis('off')\n\n    # Medium patches (index + 945)\n    axes[0, 1].imshow(images[base_index + num_samples])\n    axes[0, 1].set_title(f\"Medium Image {base_index + num_samples}\")\n    axes[0, 1].axis('off')\n    \n    axes[1, 1].imshow(masks[base_index + num_samples], cmap='gray')\n    axes[1, 1].set_title(f\"Medium Mask {base_index + num_samples}\")\n    axes[1, 1].axis('off')\n\n    axes[2, 1].imshow(heatmaps[base_index + num_samples], cmap='gray')\n    axes[2, 1].set_title(f\"Medium heatmap {base_index + num_samples}\")\n    axes[2, 1].axis('off')\n    \n    # Small patches (index + 1890)\n    axes[0, 2].imshow(images[base_index + small_sample])\n    axes[0, 2].set_title(f\"Small Image {base_index + small_sample}\")\n    axes[0, 2].axis('off')\n    \n    axes[1, 2].imshow(masks[base_index + small_sample], cmap='gray')\n    axes[1, 2].set_title(f\"Small Mask {base_index + small_sample}\")\n    axes[1, 2].axis('off')\n\n    axes[2, 2].imshow(heatmaps[base_index + small_sample], cmap='gray')\n    axes[2, 2].set_title(f\"Small heatmap {base_index + small_sample}\")\n    axes[2, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Example: Display patches for the first region (index 0)\ndisplay_multiscale_pairs(unified_images, unified_masks, unified_heatmaps, 5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize images\nunified_images = unified_images / 255.\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode masks\nunified_masks_encoded, mask_label_encoder = encode_mask(unified_masks)\n\n# After encoding masks\nunique_labels = np.unique(unified_masks_encoded)\nnumclasses = len(unique_labels)\nprint(\"Actual number of classes:\", numclasses)\n\n\n# Categorize and reshape masks\ntrain_mask_cat = categorize_and_reshape_masks(unified_masks_encoded, numclasses)\n\n\nunified_heatmaps = np.expand_dims(unified_heatmaps, axis=-1)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# compute class weights from your labels (after encode & to_categorical)\nclass_weights = np.ones((numclasses,), dtype=np.float32)\n# increase background weight (assumes index of background is wherever '0' was mapped)\nbg_index = int(np.where(mask_label_encoder.classes_ == 0)[0][0])\nclass_weights[bg_index] = 1.5  # try 1.5–2.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input shapes for the model\ninput_shape = (unified_images.shape[1], unified_images.shape[2], unified_images.shape[3])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ntf.config.optimizer.set_experimental_options({\"layout_optimizer\": False, \"model_pruner\": False})\nfrom tensorflow.keras.callbacks import (\n    ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n)\n\n# 8) Callbacks: LR scheduler, early stopping, checkpoint\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=10,\n    min_lr=1e-6,\n    verbose=1\n)\nearly_stop = EarlyStopping(\n    monitor='val_loss',\n    patience=15,\n    restore_best_weights=True,\n    verbose=1,\n    mode='min'\n)\ncheckpoint = ModelCheckpoint(\n    '/kaggle/working/Binary_model_ER_IHC.keras',\n    monitor='val_loss',\n    save_best_only=True,\n    verbose=1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\n\n\n# 5. Perform stratified split on the new labels\nX_train, X_val, y_train_mask, y_val_mask, y_train_heatmap, y_val_heatmap = train_test_split(\n    unified_images,\n    train_mask_cat,\n    unified_heatmaps,\n    test_size=0.2,\n    random_state=42\n)\n\n# 6. Verify shapes\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_val   shape:\", X_val.shape)\nprint(\"y_train mask shape:\", y_train_mask.shape)\nprint(\"y_val   mask shape:\", y_val_mask.shape)\nprint(\"y_train heatmap shape:\", y_train_heatmap.shape)\nprint(\"y_val   heatmap shape:\", y_val_heatmap.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\ntry:\n    # Detect and initialize the TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect(tpu='local')  # Kaggle-specific TPU address\n    strategy = tf.distribute.TPUStrategy(tpu)  # TPU strategy\nexcept ValueError as e:\n    # If TPU is not available, fall back to the default strategy (GPU or CPU)\n    print(\"TPU not found, using default strategy (CPU/GPU)\")\n    print(\"Error message:\", str(e))  # Print the error message\n    strategy = tf.distribute.get_strategy()\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\nGLOBAL_BS = 64   # 8 TPU cores → per-core 4\ndef ds_from_numpy(X, y_mask, y_heat):\n    ds = tf.data.Dataset.from_tensor_slices((X, {'NS': y_mask, 'CM': y_heat}))\n    ds = ds.shuffle(2048, reshuffle_each_iteration=True)\n    ds = ds.batch(GLOBAL_BS, drop_remainder=True)  # <- important on TPU\n    ds = ds.prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = ds_from_numpy(X_train, y_train_mask, y_train_heatmap)\nval_ds   = ds_from_numpy(X_val,   y_val_mask,   y_val_heatmap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After you define class_weights (NumPy)  shape: [C]\nclass_weights_tf = tf.constant(class_weights, dtype=tf.float32)\n\n@tf.keras.utils.register_keras_serializable()\ndef weighted_cce(y_true, y_pred):\n    # y_true, y_pred: [B, H, W, C], one-hot y_true\n    # reshape to [1, 1, 1, C] so it broadcasts over [B, H, W, C]\n    w = tf.reshape(class_weights_tf, (1, 1, 1, -1))  # [1,1,1,C]\n\n    # per-pixel weight = weight of the true class\n    pix_w = tf.reduce_sum(y_true * w, axis=-1)      # [B, H, W]\n\n    # standard per-pixel CE (no reduction)\n    ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)  # [B, H, W]\n\n    # weighted average:\n    # sum(ce * weight) / sum(weight) instead of plain mean\n    loss = tf.reduce_sum(ce * pix_w) / tf.reduce_sum(pix_w)\n    return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Resizing, Conv2D, BatchNormalization, Activation, MaxPooling2D, UpSampling2D, Concatenate, Add, Layer, Input, Dense, LayerNormalization, SpatialDropout2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n# import tensorflow_addons as tfa\nimport math\nfrom tensorflow.keras import mixed_precision\nmixed_precision.set_global_policy('float32')\n# Custom rotation layer\nclass RotationLayer(Layer):\n    def __init__(self, k=1, **kwargs):  # Default to 90° instead of 0°\n        super(RotationLayer, self).__init__(**kwargs)\n        self.k = k % 4  # ensure valid rotation value\n\n    def call(self, inputs):\n        # Skip rotation if k == 0 (no rotation)\n        if self.k == 0:\n            return inputs\n        return tf.image.rot90(inputs, k=self.k)\n\n\n\n# Spiral fusion block:\n# - Always uses 90°, 180°, 270° rotations\n# - Rotates ONLY the current (last) feature in level_features\n# - Skips 0°/360° (original orientation already represented by x)\ndef rotation_block(x, level_features, filters):\n    # Keep the base feature (unrotated path)\n    spiral_connections = [x]\n\n    # Use only the most recent / current feature map\n    current_feat = level_features[-1]\n\n    # Generate 90°, 180°, 270° rotated versions of the current feature\n    for k in [1, 2, 3]:  # 1→90°, 2→180°, 3→270°\n        rotated = RotationLayer(k=k)(current_feat)\n\n        # Resize to match x if necessary\n        if rotated.shape[1:3] != x.shape[1:3]:\n            rotated = Resizing(x.shape[1], x.shape[2])(rotated)\n\n        # Project to desired number of filters\n        rotated = Conv2D(filters, kernel_size=3, padding='same')(rotated)\n        spiral_connections.append(rotated)\n\n    # Fuse original + rotated features\n    return Concatenate()(spiral_connections)\n\n\n\n\n# Standard conv block\ndef conv_block(x, filters, kernel_size=3, activation='relu', drop_rate=0.0, spatial=True):\n    x = Conv2D(filters, kernel_size, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n\n    if drop_rate and drop_rate > 0:\n        x = SpatialDropout2D(drop_rate)(x) if spatial else Dropout(drop_rate)(x)\n\n    x = Conv2D(filters, kernel_size, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Activation(activation)(x)\n    return x\n# Transformer-based integration module\nclass TransformerModule(Layer):\n    def __init__(self, num_heads, key_dim, ff_dim, dropout=0.1):\n        super(TransformerModule, self).__init__()\n        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n        self.ffn = tf.keras.Sequential([\n            tf.keras.layers.Dense(ff_dim, activation='relu'),\n            tf.keras.layers.Dense(key_dim * num_heads),\n            tf.keras.layers.Dropout(dropout)\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout = tf.keras.layers.Dropout(dropout)\n    \n    def call(self, inputs):\n        batch_size, H, W, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], inputs.shape[-1]\n        x = tf.reshape(inputs, [batch_size, H * W, C])\n        attn = self.mha(x, x)\n        attn = self.dropout(attn)\n        out1 = self.layernorm1(x + attn)\n        ffn = self.ffn(out1)\n        ffn = self.dropout(ffn)\n        out2 = self.layernorm2(out1 + ffn)\n        return tf.reshape(out2, [batch_size, H, W, C])\n\n# Fuse multi-scale features and apply transformer\ndef multi_scale_integration_hub(features, filters):\n    target_h, target_w = features[-1].shape[1], features[-1].shape[2]\n    resize = Resizing(target_h, target_w)\n    fused = []\n    for feat in features:\n        r = resize(feat)\n        r = Conv2D(filters, kernel_size=1, padding='same')(r)\n        fused.append(r)\n    combined = Add()(fused)\n    combined = BatchNormalization()(combined)\n\n    num_heads = 8\n    key_dim = filters // num_heads\n    ff_dim = filters * 8\n    x = TransformerModule(num_heads, key_dim, ff_dim)(combined)\n    return conv_block(x, filters * 3)\n\n\n\n\n#RoMT-Net: A Rotation-aware Multi-Scale Transformer Network for Histopathological Segmentation and Nuclei Localization\ndef CRAFTNet(input_shape=input_shape, num_classes=numclasses):\n    \n    inputs = Input(input_shape)\n    level_features = []\n\n    # Encoder\n    x = conv_block(inputs, 32,  drop_rate=0.1)\n    level_features.append(x)\n\n    x = MaxPooling2D(2)(x)\n    x = rotation_block(x, [level_features[0]], 32)\n    x = conv_block(x, 64,  drop_rate=0.2)\n    level_features.append(x)\n\n    x = MaxPooling2D(2)(x)\n    x = rotation_block(x, level_features[:2], 64)\n    x = conv_block(x, 128,  drop_rate=0.3)\n    level_features.append(x)\n\n    x = MaxPooling2D(2)(x)\n    x = rotation_block(x, level_features[:3], 128)\n    x = conv_block(x, 256,  drop_rate=0.4)\n    level_features.append(x)\n\n    x = MaxPooling2D(2)(x)\n    x = rotation_block(x, level_features[:4], 256)\n    x = conv_block(x, 512,  drop_rate=0.5)\n\n    #Multi-scale integration\n    x = multi_scale_integration_hub(level_features + [x], 512)\n\n    # Decoder\n    x = UpSampling2D(2)(x)\n    x = rotation_block(x, level_features[:4], 256)\n    x = conv_block(x, 256,  drop_rate=0.5)\n\n    x = UpSampling2D(2)(x)\n    x = rotation_block(x, level_features[:3], 128)\n    x = conv_block(x, 128,  drop_rate=0.1)\n\n    x = UpSampling2D(2)(x)\n    x = rotation_block(x, level_features[:2], 64)\n    x = conv_block(x, 64,  drop_rate=0.2)\n\n    x = UpSampling2D(2)(x)\n    x = rotation_block(x, level_features[:1], 32)\n    x = conv_block(x, 32,  drop_rate=0.3)\n    \n    # Outputs\n    NS_out = Conv2D(num_classes, 1, activation='softmax', name='NS')(x)\n    CM_out = Conv2D(1, 1, activation='sigmoid', name='CM')(x)\n\n    return Model(inputs=inputs, outputs=[NS_out, CM_out])\n\n# Build & compile under TPU strategy\nwith strategy.scope():\n    model = CRAFTNet(input_shape=input_shape, num_classes=numclasses)\n    model.compile(\n        optimizer=Adam(), \n        loss={'NS': weighted_cce,'CM': 'binary_crossentropy'},\n        loss_weights={'NS': 1.0, 'CM': 1.0},\n        metrics={'NS': 'accuracy','CM': ['mse']}\n    )\n\nmodel.summary()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=350,\n    callbacks=[reduce_lr, early_stop, checkpoint]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nh = history.history\nepochs = range(1, len(h['loss']) + 1)\n\n# --- Pull the right series ---\nns_tr_loss   = h.get('NS_loss', [])\nns_va_loss   = h.get('val_NS_loss', [])\nns_tr_acc    = h.get('NS_accuracy', [])\nns_va_acc    = h.get('val_NS_accuracy', [])\n\ncm_tr_loss   = h.get('CM_loss', [])\ncm_va_loss   = h.get('val_CM_loss', [])\ncm_tr_mse    = h.get('CM_mse', [])\ncm_va_mse    = h.get('val_CM_mse', [])\n\ntot_tr_loss  = h['loss']\ntot_va_loss  = h['val_loss']\n\n# --- Best epochs ---\nbest_tot_epoch = int(np.argmin(tot_va_loss)) + 1 if len(tot_va_loss) else None\nbest_ns_acc_ep = int(np.argmax(ns_va_acc)) + 1  if len(ns_va_acc)  else None\nbest_ns_loss_ep= int(np.argmin(ns_va_loss)) + 1 if len(ns_va_loss) else None\nbest_cm_loss_ep= int(np.argmin(cm_va_loss)) + 1 if len(cm_va_loss) else None\nbest_cm_mse_ep = int(np.argmin(cm_va_mse)) + 1 if len(cm_va_mse) else None\n\ndef maybe_vline(ep, label):\n    if ep is not None:\n        plt.axvline(ep, color='g', linestyle='--', label=f'{label} (Epoch {ep})')\n\n# --- NS (segmentation) Accuracy ---\nif len(ns_tr_acc) and len(ns_va_acc):\n    plt.figure(figsize=(9,6))\n    plt.plot(epochs, ns_tr_acc, label='NS Training Accuracy')\n    plt.plot(epochs, ns_va_acc, label='NS Validation Accuracy')\n    maybe_vline(best_ns_acc_ep, 'Best NS Val Acc')\n    plt.title('NS (Segmentation) Accuracy')\n    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True); plt.tight_layout()\n    plt.show()\n\n# --- NS (segmentation) Loss ---\nif len(ns_tr_loss) and len(ns_va_loss):\n    plt.figure(figsize=(9,6))\n    plt.plot(epochs, ns_tr_loss, label='NS Training Loss')\n    plt.plot(epochs, ns_va_loss, label='NS Validation Loss')\n    maybe_vline(best_ns_loss_ep, 'Lowest NS Val Loss')\n    plt.title('NS (Segmentation) Loss (categorical_crossentropy)')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.tight_layout()\n    plt.show()\n\n# --- CM (centroid/heatmap) Loss ---\nif len(cm_tr_loss) and len(cm_va_loss):\n    plt.figure(figsize=(9,6))\n    plt.plot(epochs, cm_tr_loss, label='CM Training Loss')\n    plt.plot(epochs, cm_va_loss, label='CM Validation Loss')\n    maybe_vline(best_cm_loss_ep, 'Lowest CM Val Loss')\n    plt.title('CM (Centroid Map) Loss (binary_crossentropy)')\n    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.tight_layout()\n    plt.show()\n\n# --- CM (centroid/heatmap) MSE ---\nif len(cm_tr_mse) and len(cm_va_mse):\n    plt.figure(figsize=(9,6))\n    plt.plot(epochs, cm_tr_mse, label='CM Training MSE')\n    plt.plot(epochs, cm_va_mse, label='CM Validation MSE')\n    maybe_vline(best_cm_mse_ep, 'Lowest CM Val MSE')\n    plt.title('CM (Centroid Map) Mean Squared Error')\n    plt.xlabel('Epoch'); plt.ylabel('MSE'); plt.legend(); plt.grid(True); plt.tight_layout()\n    plt.show()\n\n# --- Overall (weighted) model loss ---\nplt.figure(figsize=(9,6))\nplt.plot(epochs, tot_tr_loss, label='Total Training Loss')\nplt.plot(epochs, tot_va_loss, label='Total Validation Loss')\nmaybe_vline(best_tot_epoch, 'Lowest Total Val Loss')\nplt.title('Overall Model Loss (NS + 0.8×CM)')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True); plt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import tensorflow as tf\n# from tensorflow.keras.layers import Layer\n# @tf.keras.utils.register_keras_serializable()\n# def weighted_cce(y_true, y_pred):\n#     w = tf.reshape(class_weights_tf, (1, 1, 1, -1))\n#     pix_w = tf.reduce_sum(y_true * w, axis=-1)\n#     ce = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n#     loss = tf.reduce_sum(ce * pix_w) / tf.reduce_sum(pix_w)\n#     return loss\n\n\n# @tf.keras.utils.register_keras_serializable()\n# class RotationLayer(Layer):\n#     def __init__(self, k=0, **kwargs):\n#         super(RotationLayer, self).__init__(**kwargs)\n#         self.k = k\n\n#     def call(self, inputs):\n#         return tf.image.rot90(inputs, k=self.k)\n\n#     def get_config(self):\n#         config = super().get_config()\n#         config.update({'k': self.k})\n#         return config\n\n# @tf.keras.utils.register_keras_serializable()\n# class TransformerModule(Layer):\n#     def __init__(self, num_heads, key_dim, ff_dim, dropout=0.1, **kwargs):\n#         super(TransformerModule, self).__init__(**kwargs)\n#         self.num_heads = num_heads\n#         self.key_dim = key_dim\n#         self.ff_dim = ff_dim\n#         self.dropout_rate = dropout\n\n#         self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n#         self.ffn = tf.keras.Sequential([\n#             tf.keras.layers.Dense(ff_dim, activation='relu'),\n#             tf.keras.layers.Dense(key_dim * num_heads),\n#             tf.keras.layers.Dropout(dropout)\n#         ])\n#         self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n#         self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n#         self.dropout = tf.keras.layers.Dropout(dropout)\n\n#     def call(self, inputs):\n#         batch_size, H, W, C = tf.shape(inputs)[0], tf.shape(inputs)[1], tf.shape(inputs)[2], inputs.shape[-1]\n#         x = tf.reshape(inputs, [batch_size, H * W, C])\n#         attn = self.mha(x, x)\n#         attn = self.dropout(attn)\n#         out1 = self.layernorm1(x + attn)\n#         ffn = self.ffn(out1)\n#         ffn = self.dropout(ffn)\n#         out2 = self.layernorm2(out1 + ffn)\n#         return tf.reshape(out2, [batch_size, H, W, C])\n\n#     def get_config(self):\n#         config = super().get_config()\n#         config.update({\n#             'num_heads': self.num_heads,\n#             'key_dim': self.key_dim,\n#             'ff_dim': self.ff_dim,\n#             'dropout': self.dropout_rate\n#         })\n#         return config\n\n\n# model = load_model(\n#     '/kaggle/input/deletethismodel/other/default/1/Binary_model_ER_IHC (2).keras',\n#     custom_objects={\n#         'RotationLayer': RotationLayer,\n#         'TransformerModule': TransformerModule,\n#         'weighted_cce': weighted_cce,   # optional if registered, but safe\n#     },\n#     compile=True\n# )\n\n# # # Export to a TF SavedModel for broad TF compatibility\n# # model.export(\"MOSAIC_savedmodel\", format=\"tf_saved_model\")\n# # # (or also create an H5 if your layers are compatible)\n# # model.save(\"MOSAIC.h5\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.feature import peak_local_max\nfrom skimage.segmentation import watershed\nfrom scipy.ndimage import distance_transform_edt\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport random\nfrom skimage.measure import label, regionprops\nfrom scipy.stats import mode\n#Color map for ER\nCOLOR_MAPPING = {\n    0: [0, 0, 0],        # Black (Background)\n    122: [0, 159, 255],  # Blue (Normal)\n    150: [0, 255, 0],    # Green (Weak)\n    203: [255, 216, 0],  # Yellow (Moderate)\n    76: [255, 0, 0]      # Red (Strong)\n}\n\n# #For MonuSac\n# COLOR_MAPPING = {\n#     0:   [  0,   0,   0],  # Black   (Background)\n#     29:  [  0,   0, 255],  # Blue    (Neutrophils)\n#     76:  [255,   0,   0],  # Red     (Epithelial)\n#     150: [  0, 255,   0],  # Green   (Macrophages)\n#     179: [  0, 255, 255],  # Cyan    (Moderate)\n# }\n\n# # === Final 4-Class Color Mapping ===\n# COLOR_MAPPING = {\n#     0:   [  0,   0,   0],     # Background - Black\n#     76:  [255,   0,   0],     # Epithelial - Red\n#     104: [  0, 128, 255],     # Inflammatory - Blue\n#     151: [255, 128,   0],     # Spindle-shaped - Orange\n# }\n\ndef separate_nuclei(seg_mask, heatmap_pred, min_distance=5):\n    \"\"\"Post-processing using predicted centroids (single channel heatmap)\"\"\"\n    # Find peaks in single channel heatmap\n    peaks = peak_local_max(heatmap_pred, \n                          min_distance=min_distance,\n                          threshold_abs=0.3)\n    \n    # Create markers for watershed\n    all_markers = np.zeros_like(seg_mask)\n    for i, (y, x) in enumerate(peaks):\n        all_markers[y, x] = i + 1  # Unique labels for each nucleus\n        \n    # Apply watershed\n    distance_map = distance_transform_edt(seg_mask)\n    labels = watershed(-distance_map, all_markers, mask=seg_mask)\n    \n    return labels\n\n\ndef apply_fixed_colormap(mask):\n    \"\"\"Map grayscale mask values to RGB colors using fixed colormap.\"\"\"\n    colored = np.zeros((*mask.shape, 3), dtype=np.uint8)\n\n    for gray_value, rgb_color in COLOR_MAPPING.items():\n        colored[mask == gray_value] = rgb_color\n\n    return colored\ndef refine_segmentation(pred_mask):\n    \"\"\"Assign one class per nucleus using majority voting.\"\"\"\n    refined_mask = np.zeros_like(pred_mask)\n    binary_mask = pred_mask > 0  # Nuclei are non-zero classes\n    labeled_mask = label(binary_mask)\n\n    for region in regionprops(labeled_mask):\n        coords = region.coords\n        values = pred_mask[coords[:, 0], coords[:, 1]]\n        majority_class = mode(values, axis=None).mode.item()\n        refined_mask[coords[:, 0], coords[:, 1]] = majority_class\n\n    return refined_mask\n    \n\n\n# Select a random test image from validation set\ntest_img_number = random.randint(0, len(unified_images) - 1)\nscale=\"scale\"\nif 0 <= test_img_number < num_samples:\n    scale=\"20X scale\"\nelif num_samples <= test_img_number <= (num_samples+num_samples):\n    scale=\"40X scale\"\nelif (num_samples+num_samples) <= test_img_number <= (num_samples+num_samples+num_samples):\n    scale=\"80X scale\"\ntest_img = unified_images[test_img_number]\nground_truth_seg = train_mask_cat[test_img_number]\nground_truth_heatmap = unified_heatmaps[test_img_number]\n\n\n\n# Prepare input for prediction\ntest_img_input = np.expand_dims(test_img, axis=0)\n\n# Get predictions\nprediction = model.predict(test_img_input)\nseg_pred = prediction[0][0]  # Segmentation output\nheatmap_pred = prediction[1][0]  # Heatmap output (now single channel)\n\n\n# Process segmentation outputs\nground_truth_encoded = np.argmax(ground_truth_seg, axis=-1)\nground_truth_class = mask_label_encoder.inverse_transform(ground_truth_encoded.ravel()).reshape(ground_truth_encoded.shape)\npredicted_encoded = np.argmax(seg_pred, axis=-1)\npredicted_class = mask_label_encoder.inverse_transform(predicted_encoded.ravel()).reshape(predicted_encoded.shape)\npredicted_class = refine_segmentation(predicted_class)\n# Prepare test image for display\ntest_img_display = (test_img * 255).astype(np.uint8)\ntest_img_display = cv2.cvtColor(test_img_display, cv2.COLOR_BGR2RGB)\n\n# Create contour visualization\ncontoured_image = test_img_display.copy()\ncontour_mask = predicted_class.astype(np.uint8)\ncontours, _ = cv2.findContours(contour_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n\n\nfor contour in contours:\n    M = cv2.moments(contour)\n    if M[\"m00\"] == 0:\n        continue\n    cx = int(M[\"m10\"] / M[\"m00\"])\n    cy = int(M[\"m01\"] / M[\"m00\"])\n    if 0 <= cy < predicted_class.shape[0] and 0 <= cx < predicted_class.shape[1]:\n        class_label = predicted_class[cy, cx]\n        color = tuple(COLOR_MAPPING.get(class_label, (0, 0, 0)))\n        cv2.drawContours(contoured_image, [contour], -1, color, 2)\n\n\n\n# Post-process predictions\nseparated_instances = separate_nuclei(\n    seg_mask=np.argmax(seg_pred, axis=-1),\n    heatmap_pred=np.squeeze(heatmap_pred),\n    min_distance=5\n)\n\n# Create colored instances visualization that maintains class colors\ncolored_instances = apply_fixed_colormap(predicted_class).copy()\n\n# Draw instance boundaries on top of class-colored image\ninstance_boundaries = np.zeros_like(predicted_class)\nfor label in np.unique(separated_instances):\n    if label == 0:  # Skip background\n        continue\n    mask = np.uint8(separated_instances == label)\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    cv2.drawContours(colored_instances, contours, -1, (0, 0, 0), 1)  # White boundaries\n\n\n##################################\n# Create contour visualization from colored_instances\ncontoured_from_colored = test_img_display.copy()\n\n# Convert colored_instances to grayscale to find contours\ncolored_gray = cv2.cvtColor(colored_instances, cv2.COLOR_BGR2GRAY)\ncontours, _ = cv2.findContours(colored_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n# Draw contours with class colors\nfor contour in contours:\n    M = cv2.moments(contour)\n    if M[\"m00\"] == 0:\n        continue\n    cx = int(M[\"m10\"] / M[\"m00\"])\n    cy = int(M[\"m01\"] / M[\"m00\"])\n    if 0 <= cy < predicted_class.shape[0] and 0 <= cx < predicted_class.shape[1]:\n        class_label = predicted_class[cy, cx]\n        color = tuple(COLOR_MAPPING.get(class_label, (0, 0, 0)))\n        cv2.drawContours(contoured_from_colored, [contour], -1, color, 2)\n\n##################################\n\n# Create figure with modified layout\nplt.figure(figsize=(24, 18))\n\n# First row: Input and segmentation results\nplt.subplot(3, 3, 1)\nplt.title('Testing Image ' + scale)\nplt.imshow(test_img_display)\nplt.axis('off')\n\nplt.subplot(3, 3, 2)\nplt.title('Segmentation Ground Truth ' + scale)\nplt.imshow(apply_fixed_colormap(ground_truth_class))\nplt.axis('off')\n\nplt.subplot(3, 3, 3)\nplt.title('Predicted Segmentation ' + scale)\nplt.imshow(apply_fixed_colormap(predicted_class))\nplt.axis('off')\n\n# Second row: Heatmaps and contours\nplt.subplot(3, 3, 4)\nplt.title('Coarse Contours on Image ' + scale)\nplt.imshow(contoured_image)\nplt.axis('off')\n\nplt.subplot(3, 3, 5)\nplt.title('Centroid Ground Truth ' + scale)\nplt.imshow(np.squeeze(ground_truth_heatmap), cmap='gray')\nplt.axis('off')\n\nplt.subplot(3, 3, 6)\nplt.title('Predicted centroids ' + scale)\nplt.imshow(np.squeeze(heatmap_pred), cmap='gray')\nplt.axis('off')\n\n# Third row: Instance separation results\nplt.subplot(3, 3, 7)\nplt.title('Watershed Labels (For Debug) ' + scale)\nplt.imshow(separated_instances, cmap='nipy_spectral')\nplt.axis('off')\n\nplt.subplot(3, 3, 8)\nplt.title('Separated Instances with Class Colors ' + scale)\nplt.imshow(colored_instances)\nplt.axis('off')\n\n\nplt.subplot(3, 3, 9)\nplt.title('Instance Contours on Image ' + scale)\nplt.imshow(contoured_from_colored)\nplt.axis('off')\n\n\nplt.tight_layout()\nplt.show()\n\n# Diagnostic information\nprint(f\"Displaying results for sample {test_img_number}\")\nprint(\"Unique labels in ground truth:\", np.unique(ground_truth_class))\nprint(\"Unique labels in prediction:\", np.unique(predicted_class))\nprint(\"Number of detected instances:\", len(np.unique(separated_instances))-1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Code to save only final segmentation**","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.measure import label as sk_label, regionprops\nfrom scipy.stats import mode   # make sure this is imported\n\ndef refine_segmentation(pred_mask):\n    refined_mask = np.zeros_like(pred_mask)\n    binary_mask = pred_mask > 0\n    labeled_mask = sk_label(binary_mask)   # use the alias\n    for region in regionprops(labeled_mask):\n        coords = region.coords\n        values = pred_mask[coords[:, 0], coords[:, 1]]\n        majority_class = mode(values, axis=None).mode.item()\n        refined_mask[coords[:, 0], coords[:, 1]] = majority_class\n    return refined_mask\n\n# === where to save on Kaggle ===\nout_dir = \"/kaggle/working/romtnet_ER_Segmentation\"\nos.makedirs(out_dir, exist_ok=True)\n\ndef determine_scale(idx, num_samples):\n    if 0 <= idx < num_samples:\n        return \"20X scale\"\n    elif num_samples <= idx < 2 * num_samples:\n        return \"40X scale\"\n    else:\n        return \"80X scale\"\n\n# total number of images to process\ntotal_images = len(unified_images)\n\nfor idx, (test_img, ground_truth_seg, ground_truth_heatmap) in enumerate(\n    zip(unified_images, train_mask_cat, unified_heatmaps)\n):\n    scale = determine_scale(idx, num_samples)\n\n    # Prepare input and predict\n    test_img_input = np.expand_dims(test_img, axis=0)\n    prediction = model.predict(test_img_input, verbose=0)\n    seg_pred = prediction[0][0]        # Segmentation output [H,W,C]\n    heatmap_pred = prediction[1][0]    # Heatmap output [H,W,1] or [H,W]\n\n    # Convert one-hot (or logits) to class map via label encoder\n    gt_encoded = np.argmax(ground_truth_seg, axis=-1)\n    ground_truth_class = mask_label_encoder.inverse_transform(\n        gt_encoded.ravel()\n    ).reshape(gt_encoded.shape)\n\n    pred_encoded = np.argmax(seg_pred, axis=-1)\n    predicted_class = mask_label_encoder.inverse_transform(\n        pred_encoded.ravel()\n    ).reshape(pred_encoded.shape)\n\n    # Majority vote refinement per instance\n    predicted_class = refine_segmentation(predicted_class)\n\n    # Prepare RGB image for overlays/plots\n    test_img_display = (test_img * 255).astype(np.uint8)\n    # If 'test_img' was originally RGB scaled [0,1], this conversion is correct:\n    # matplotlib expects RGB, so keep it as RGB for plotting/drawing\n    test_img_display = cv2.cvtColor(test_img_display, cv2.COLOR_BGR2RGB)\n\n    # Coarse contour visualization (by class colors at centroid)\n    contoured_image = test_img_display.copy()\n    contour_mask = predicted_class.astype(np.uint8)\n    contours, _ = cv2.findContours(contour_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    for contour in contours:\n        M = cv2.moments(contour)\n        if M[\"m00\"] == 0:\n            continue\n        cx = int(M[\"m10\"] / M[\"m00\"])\n        cy = int(M[\"m01\"] / M[\"m00\"])\n        if 0 <= cy < predicted_class.shape[0] and 0 <= cx < predicted_class.shape[1]:\n            class_label = predicted_class[cy, cx]\n            color = tuple(COLOR_MAPPING.get(class_label, (0, 0, 0)))\n            cv2.drawContours(contoured_image, [contour], -1, color, 2)\n\n    # Instance separation from seg + centroid heatmap\n    separated_instances = separate_nuclei(\n        seg_mask=np.argmax(seg_pred, axis=-1),\n        heatmap_pred=np.squeeze(heatmap_pred),\n        min_distance=5\n    )\n\n    # Colored instances with class colors + black boundaries\n    colored_instances = apply_fixed_colormap(predicted_class).copy()\n    for inst_label in np.unique(separated_instances):\n        if inst_label == 0:\n            continue\n        mask = np.uint8(separated_instances == inst_label)\n        inst_contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n        cv2.drawContours(colored_instances, inst_contours, -1, (0, 0, 0), 1)\n\n    # Contours drawn directly from class-colored instances\n    contoured_from_colored = test_img_display.copy()\n    colored_gray = cv2.cvtColor(colored_instances, cv2.COLOR_BGR2GRAY)\n    contours2, _ = cv2.findContours(colored_gray, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    for contour in contours2:\n        M = cv2.moments(contour)\n        if M[\"m00\"] == 0:\n            continue\n        cx = int(M[\"m10\"] / M[\"m00\"])\n        cy = int(M[\"m01\"] / M[\"m00\"])\n        if 0 <= cy < predicted_class.shape[0] and 0 <= cx < predicted_class.shape[1]:\n            class_label = predicted_class[cy, cx]\n            color = tuple(COLOR_MAPPING.get(class_label, (0, 0, 0)))\n            cv2.drawContours(contoured_from_colored, [contour], -1, color, 2)\n\n    # ===== Save ONLY the contoured_from_colored as 512x512x3 PNG =====\n    # contoured_from_colored is RGB now (for matplotlib). Convert to BGR for cv2.imwrite.\n    cfc_resized = cv2.resize(contoured_from_colored, (512, 512), interpolation=cv2.INTER_LINEAR)\n    cfc_bgr = cv2.cvtColor(cfc_resized, cv2.COLOR_RGB2BGR)\n    out_img_path = os.path.join(out_dir, f\"contoured_{idx:05d}_{scale.replace(' ', '')}.png\")\n    cv2.imwrite(out_img_path, cfc_bgr)\n\n\n\n    # === Print progress ===\n    print(f\"Processed {idx+1}/{total_images} images\", end='\\r', flush=True)\n\nprint(f\"\\nSaved PNGs to: {out_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Zip and Download**","metadata":{}},{"cell_type":"code","source":"!apt-get install -y zip\n!zip -r ERReults.zip /kaggle/working/romtnet_ER_Segmentation","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}